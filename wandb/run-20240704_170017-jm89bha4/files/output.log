[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 17:00:22 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 17:00:22 - INFO - __main__ - ==> Preparing data..
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.17ba/s]
Dataset: restaurant_sup
Size of training set: 3452
Size of testing set: 1120
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.85ba/s]
Sample 3388 of the training set: {'labels': tensor(1), 'input_ids': tensor([    0, 19487,     9,  3984,  1437,     2,  1213,    74,  3999,   190,
          905,   162,  2073,   127,  4049,     9,  3984,   137,  1839,   277,
            4,     2,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>glass of wine </s>They wouldnt even let me finish my glass of wine before offering another.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 17:00:26 - INFO - __main__ - Sample 3388 of the training set: {'labels': tensor(1), 'input_ids': tensor([    0, 19487,     9,  3984,  1437,     2,  1213,    74,  3999,   190,
          905,   162,  2073,   127,  4049,     9,  3984,   137,  1839,   277,
            4,     2,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>glass of wine </s>They wouldnt even let me finish my glass of wine before offering another.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 17:00:26 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 17:00:31 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 17:00:31 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/restaurant_unsup_roberta/,  Dataset name = restaurant_sup, seed = 111
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(

  4%|██████▋                                                                                                                                                          | 9/216 [00:03<00:38,  5.34it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:18<00:00, 11.81it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.6002, training loss = 0.0583

 12%|██████████████████▌                                                                                                                                             | 25/216 [00:03<00:14, 13.30it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 211/216 [00:16<00:00, 13.77it/s]
train acc = 0.7645, training loss = 0.0364
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.70it/s]
  0%|▋                                                                                                                                                                | 1/216 [00:01<05:44,  1.60s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.50it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.8195, training loss = 0.0283

  7%|███████████                                                                                                                                                     | 15/216 [00:02<00:18, 10.98it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.59it/s]
  2%|███▋                                                                                                                                                             | 5/216 [00:01<00:44,  4.73it/s]
train acc = 0.8592, training loss = 0.0219
Epoch 4 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:16<00:00, 12.82it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.8957, training loss = 0.0171

  4%|██████▋                                                                                                                                                          | 9/216 [00:02<00:33,  6.14it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.18it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9279, training loss = 0.0118

  6%|█████████▋                                                                                                                                                      | 13/216 [00:02<00:22,  8.92it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.15it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9577, training loss = 0.0079

 10%|███████████████▌                                                                                                                                                | 21/216 [00:03<00:15, 12.40it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.23it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9606, training loss = 0.0067
Epoch 8 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 215/216 [00:17<00:00, 13.77it/s]
train acc = 0.9748, training loss = 0.0048
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.34it/s]
  3%|█████▏                                                                                                                                                           | 7/216 [00:01<00:36,  5.79it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.64it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9794, training loss = 0.0037

 10%|███████████████▌                                                                                                                                                | 21/216 [00:02<00:15, 12.77it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight






100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.62it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9780, training loss = 0.0036

  3%|█████▏                                                                                                                                                           | 7/216 [00:01<00:37,  5.55it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.54it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9867, training loss = 0.0026

 10%|███████████████▌                                                                                                                                                | 21/216 [00:02<00:15, 12.77it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 215/216 [00:16<00:00, 13.79it/s]
train acc = 0.9904, training loss = 0.0017
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.63it/s]
  1%|██▏                                                                                                                                                              | 3/216 [00:01<01:40,  2.13it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.50it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9904, training loss = 0.0016

  8%|████████████▌                                                                                                                                                   | 17/216 [00:02<00:16, 11.88it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.65it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9904, training loss = 0.0018
Epoch 15 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.53it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9896, training loss = 0.0016

  3%|█████▏                                                                                                                                                           | 7/216 [00:01<00:38,  5.50it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.54it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9925, training loss = 0.0014

  8%|████████████▌                                                                                                                                                   | 17/216 [00:02<00:17, 11.52it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 211/216 [00:16<00:00, 13.76it/s]
train acc = 0.9919, training loss = 0.0013
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.44it/s]
  0%|▋                                                                                                                                                                | 1/216 [00:01<06:06,  1.70s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.43it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9948, training loss = 0.0008

  3%|█████▏                                                                                                                                                           | 7/216 [00:02<00:43,  4.82it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.27it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9945, training loss = 0.0011

 10%|███████████████▌                                                                                                                                                | 21/216 [00:02<00:15, 12.76it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.59it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9962, training loss = 0.0006

 13%|█████████████████████▍                                                                                                                                          | 29/216 [00:03<00:13, 13.42it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 213/216 [00:17<00:00, 13.78it/s]
train acc = 0.9933, training loss = 0.0010
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.22it/s]
  2%|███▋                                                                                                                                                             | 5/216 [00:01<00:53,  3.98it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.58it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9954, training loss = 0.0007

 10%|███████████████▌                                                                                                                                                | 21/216 [00:02<00:15, 12.84it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 215/216 [00:16<00:00, 13.78it/s]
train acc = 0.9959, training loss = 0.0005
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.70it/s]
  1%|██▏                                                                                                                                                              | 3/216 [00:01<01:44,  2.04it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.45it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9948, training loss = 0.0007

  6%|█████████▋                                                                                                                                                      | 13/216 [00:02<00:21,  9.65it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.47it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9974, training loss = 0.0005

 12%|██████████████████▌                                                                                                                                             | 25/216 [00:03<00:14, 13.18it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight






100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.46it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9965, training loss = 0.0005

  5%|████████▏                                                                                                                                                       | 11/216 [00:02<00:22,  8.93it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.64it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9962, training loss = 0.0004

  6%|█████████▋                                                                                                                                                      | 13/216 [00:02<00:21,  9.37it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.35it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9942, training loss = 0.0006
Epoch 29 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.46it/s]
  0%|                                                                                                                                                                          | 0/70 [00:00<?, ?it/s]

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:03<00:00, 18.86it/s]
07/04/2024 17:09:15 - INFO - approaches.finetune - .//seq0/640000samples/lora_init/restaurant_unsup_roberta/ On restaurant_sup, last epoch macro_f1 = 0.7818, acc = 0.8580 (seed=111)
Path of progressive f1 score: .//seq0/640000samples/lora_init/restaurant_unsup_roberta//../lora_piggyback/progressive_f1_111
Path of progressive accuracy: .//seq0/640000samples/lora_init/restaurant_unsup_roberta//../lora_piggyback/progressive_acc_111