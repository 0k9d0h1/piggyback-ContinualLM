[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 18:54:19 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 18:54:19 - INFO - __main__ - ==> Preparing data..
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.15ba/s]
total_num:  1688
len(new_data['test']['labels']):  253
Dataset: aclarc_sup
Size of training set: 1216
Size of testing set: 304
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.16ba/s]
Sample 827 of the training set: {'labels': tensor(0), 'input_ids': tensor([    0,  6785,    25,  2773,  2156,    52,    64,  1421,  3104,  3505,
           14, 25962,    19, 11410,    11,    41,    15,    12,  1902, 33900,
        36451, 12712,    31,   167,    14,   109,    45,    36, 46470, 47980,
         1547,  4400,  1076,     4,  9095,  4839,   479,     2,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX Brown et al. 1993 ).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 18:54:22 - INFO - __main__ - Sample 827 of the training set: {'labels': tensor(0), 'input_ids': tensor([    0,  6785,    25,  2773,  2156,    52,    64,  1421,  3104,  3505,
           14, 25962,    19, 11410,    11,    41,    15,    12,  1902, 33900,
        36451, 12712,    31,   167,    14,   109,    45,    36, 46470, 47980,
         1547,  4400,  1076,     4,  9095,  4839,   479,     2,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>Just as easily, we can model link types that coincide with entries in an on-line bilingual dictionary separately from those that do not ( cfXXX Brown et al. 1993 ).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 18:54:22 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 18:54:29 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 18:54:29 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/acl_unsup_roberta/,  Dataset name = aclarc_sup, seed = 2021
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): LoRARobertaClassificationHead(
      (dense): PretrainingMultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=768, bias=True)
          (1): Linear(in_features=768, out_features=768, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): MultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=6, bias=True)
          (1): Linear(in_features=768, out_features=6, bias=True)
        )
      )
    )
  )
  (sigmoid): Sigmoid()
  (mse_loss): MSELoss()
  (cos): CosineSimilarity()
  (tanh): Tanh()
  (softmax): Softmax(dim=1)
  (kd_loss): DistillKL()
  (dropout): Dropout(p=0.1, inplace=False)
  (contrast): MyContrastive(
    (bce): BCEWithLogitsLoss()
    (ce): CrossEntropyLoss()
    (sup_con): SupConLoss()
  )
)
summary_path: .//seq0/640000samples/lora_init/acl_unsup_roberta/../lora_piggyback/aclarc_sup_finetune_summary
Epoch 0 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:08<00:00,  9.35it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.4877, training loss = 0.0934

 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                              | 9/76 [00:02<00:10,  6.62it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.38it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.5551, training loss = 0.0800

 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                        | 19/76 [00:03<00:04, 11.97it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 73/76 [00:07<00:00, 13.78it/s]
train acc = 0.5551, training loss = 0.0777
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.32it/s]
  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                           | 3/76 [00:01<00:35,  2.07it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.59it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.6488, training loss = 0.0644

 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                     | 13/76 [00:02<00:06,  9.51it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.52it/s]
  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                           | 3/76 [00:01<00:28,  2.52it/s]
train acc = 0.7163, training loss = 0.0530
Epoch 5 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:06<00:00, 11.11it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.7607, training loss = 0.0457

 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                 | 15/76 [00:02<00:05, 10.66it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.55it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.8355, training loss = 0.0334
Epoch 7 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.31it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.8849, training loss = 0.0247

  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                                                   | 7/76 [00:02<00:13,  5.07it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.44it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9227, training loss = 0.0172

 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                    | 21/76 [00:02<00:04, 12.74it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.84it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9400, training loss = 0.0119

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                                                       | 5/76 [00:02<00:19,  3.57it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.51it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9613, training loss = 0.0085

 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                     | 13/76 [00:02<00:06,  9.21it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.22it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9729, training loss = 0.0072
Epoch 12 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.69it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9720, training loss = 0.0056

 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                         | 11/76 [00:02<00:07,  8.57it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.72it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9836, training loss = 0.0042
Epoch 14 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:06<00:00, 11.04it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9827, training loss = 0.0037

 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                         | 11/76 [00:02<00:07,  8.36it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.57it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9860, training loss = 0.0034
Epoch 16 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.71it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9877, training loss = 0.0027

  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                                                       | 5/76 [00:02<00:21,  3.36it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.30it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9836, training loss = 0.0029

 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                        | 19/76 [00:02<00:04, 12.27it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 73/76 [00:06<00:00, 13.77it/s]
train acc = 0.9893, training loss = 0.0026
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.73it/s]
  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                           | 3/76 [00:01<00:34,  2.10it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.62it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9893, training loss = 0.0024

  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                                                   | 7/76 [00:02<00:13,  5.19it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00, 10.52it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9852, training loss = 0.0025
Epoch 21 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:06<00:00, 11.06it/s]
  0%|                                                                                                                                                                          | 0/76 [00:00<?, ?it/s]
train acc = 0.9918, training loss = 0.0020

  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                                                   | 7/76 [00:02<00:13,  5.17it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.1
n,pï¼š  model.classifier.dense.classifiers.1.weight
n,pï¼š  model.classifier.dense.classifiers.1.bias
n,pï¼š  model.classifier.out_proj.classifiers.1.weight

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                  | 37/76 [00:04<00:02, 13.67it/s]Traceback (most recent call last):
  File "finetune.py", line 150, in <module>
    main()
  File "finetune.py", line 146, in main
    appr.train(model, accelerator, train_loader, test_loader)
  File "/home/0k9d0h1/piggyback/piggyback-ContinualLM/approaches/finetune.py", line 80, in train
    model, optimizer, train_loader, accelerator, lr_scheduler)
  File "/home/0k9d0h1/piggyback/piggyback-ContinualLM/approaches/finetune.py", line 198, in train_epoch
    optimizer.step()
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/accelerate/optimizer.py", line 134, in step
    self.optimizer.step(closure)
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py", line 359, in step
    exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))
KeyboardInterrupt