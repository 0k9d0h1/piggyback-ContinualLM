[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 18:46:42 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 18:46:42 - INFO - __main__ - ==> Preparing data..
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.55ba/s]
Dataset: restaurant_sup
Size of training set: 2761
Size of testing set: 691
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.66ba/s]
Sample 1655 of the training set: {'labels': tensor(0), 'input_ids': tensor([    0,    90,  4989, 27468,   271, 24959,  6315,  1437,     2, 16837,
        28099, 27468,   271, 24959,  6315,    16,     7,  1597,    13,     4,
            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>tuna tartar appetizer </s>Their tuna tartar appetizer is to die for.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 18:46:46 - INFO - __main__ - Sample 1655 of the training set: {'labels': tensor(0), 'input_ids': tensor([    0,    90,  4989, 27468,   271, 24959,  6315,  1437,     2, 16837,
        28099, 27468,   271, 24959,  6315,    16,     7,  1597,    13,     4,
            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>tuna tartar appetizer </s>Their tuna tartar appetizer is to die for.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 18:46:46 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 18:46:51 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 18:46:51 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/restaurant_unsup_roberta/,  Dataset name = restaurant_sup, seed = 2021
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): LoRARobertaClassificationHead(
      (dense): PretrainingMultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=768, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): MultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=3, bias=True)
        )
      )
    )
  )
  (sigmoid): Sigmoid()
  (mse_loss): MSELoss()
  (cos): CosineSimilarity()
  (tanh): Tanh()
  (softmax): Softmax(dim=1)
  (kd_loss): DistillKL()
  (dropout): Dropout(p=0.1, inplace=False)
  (contrast): MyContrastive(
    (bce): BCEWithLogitsLoss()
    (ce): CrossEntropyLoss()
    (sup_con): SupConLoss()
  )
)
summary_path: .//seq0/640000samples/lora_init/restaurant_unsup_roberta/../lora_piggyback/restaurant_sup_finetune_summary

  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                        | 9/173 [00:02<00:29,  5.58it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 11.55it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.5911, training loss = 0.0600

  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                                                            | 5/173 [00:02<00:47,  3.51it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.09it/s]
  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                                                            | 5/173 [00:01<00:44,  3.80it/s]
train acc = 0.7081, training loss = 0.0439
Epoch 2 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.24it/s]
  2%|â–ˆâ–ˆâ–Š                                                                                                                                                              | 3/173 [00:01<01:21,  2.10it/s]
train acc = 0.7845, training loss = 0.0336
Epoch 3 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.20it/s]
  5%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                        | 9/173 [00:01<00:20,  8.17it/s]
train acc = 0.8330, training loss = 0.0267
Epoch 4 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:13<00:00, 12.64it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.8649, training loss = 0.0217

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                          | 7/173 [00:02<00:32,  5.12it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.10it/s]
  2%|â–ˆâ–ˆâ–Š                                                                                                                                                              | 3/173 [00:01<01:29,  1.89it/s]
train acc = 0.9073, training loss = 0.0151
Epoch 6 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.05it/s]
  1%|â–‰                                                                                                                                                                | 1/173 [00:01<04:36,  1.61s/it]
train acc = 0.9326, training loss = 0.0112
Epoch 7 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.22it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9504, training loss = 0.0088
Epoch 8 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.35it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9623, training loss = 0.0063
Epoch 9 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.27it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9786, training loss = 0.0038
Epoch 10 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.25it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight
n,pï¼š  model.classifier.out_proj.classifiers.0.bias
train acc = 0.9841, training loss = 0.0031

 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                         | 25/173 [00:03<00:11, 13.25it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:13<00:00, 12.40it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9844, training loss = 0.0029

 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                          | 23/173 [00:03<00:11, 12.94it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.20it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9841, training loss = 0.0026

 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                         | 25/173 [00:03<00:11, 13.30it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:13<00:00, 12.43it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9913, training loss = 0.0018

 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                         | 25/173 [00:03<00:11, 13.18it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.21it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9928, training loss = 0.0016

 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                                          | 23/173 [00:03<00:11, 12.94it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.19it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9920, training loss = 0.0014

 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                              | 19/173 [00:03<00:12, 12.00it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.03it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9928, training loss = 0.0012

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                    | 13/173 [00:02<00:15, 10.05it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.35it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9938, training loss = 0.0011

  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                    | 13/173 [00:02<00:16,  9.98it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.33it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9942, training loss = 0.0011

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                     | 11/173 [00:02<00:19,  8.28it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.15it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9960, training loss = 0.0007

  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                                     | 11/173 [00:02<00:19,  8.50it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.21it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9964, training loss = 0.0007

  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                          | 7/173 [00:02<00:33,  5.00it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.03it/s]
  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                                                            | 5/173 [00:01<00:43,  3.89it/s]
train acc = 0.9960, training loss = 0.0006
Epoch 22 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.29it/s]
  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                                          | 7/173 [00:01<00:29,  5.65it/s]
train acc = 0.9964, training loss = 0.0005
Epoch 23 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.32it/s]

  3%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                                                            | 5/173 [00:01<00:44,  3.80it/s]
train acc = 0.9971, training loss = 0.0004
Epoch 24 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.25it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9957, training loss = 0.0006
Epoch 25 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight






100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.32it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9964, training loss = 0.0004
Epoch 26 started
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0

n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.20it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9975, training loss = 0.0003

 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                            | 21/173 [00:03<00:12, 12.44it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 11.98it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9989, training loss = 0.0003

 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                                            | 21/173 [00:03<00:11, 12.68it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.27it/s]
  0%|                                                                                                                                                                         | 0/173 [00:00<?, ?it/s]
train acc = 0.9978, training loss = 0.0003

 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                              | 19/173 [00:02<00:12, 12.18it/s]
n,pï¼š  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.0.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.1.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.2.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.3.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.4.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.5.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.6.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.7.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.8.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.9.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.10.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,pï¼š  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,pï¼š  model.roberta.encoder.layer.11.output.dense.masks.0
n,pï¼š  model.classifier.dense.classifiers.0.weight
n,pï¼š  model.classifier.dense.classifiers.0.bias
n,pï¼š  model.classifier.out_proj.classifiers.0.weight





100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:14<00:00, 12.20it/s]
  0%|                                                                                                                                                                          | 0/44 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44/44 [00:02<00:00, 16.05it/s]
07/04/2024 18:53:59 - INFO - approaches.finetune - .//seq0/640000samples/lora_init/restaurant_unsup_roberta/ On restaurant_sup, last epoch macro_f1 = 0.7770, acc = 0.8480 (seed=2021)
Path of progressive f1 score: .//seq0/640000samples/lora_init/restaurant_unsup_roberta//../lora_piggyback/progressive_f1_2021
Path of progressive accuracy: .//seq0/640000samples/lora_init/restaurant_unsup_roberta//../lora_piggyback/progressive_acc_2021