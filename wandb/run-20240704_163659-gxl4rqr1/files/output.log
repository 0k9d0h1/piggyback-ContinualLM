[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 16:37:04 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 16:37:04 - INFO - __main__ - ==> Preparing data..
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.20ba/s]
total_num:  1688
len(new_data['test']['labels']):  253
Dataset: aclarc_sup
Size of training set: 1520
Size of testing set: 421
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.61ba/s]
Sample 827 of the training set: {'labels': tensor(5), 'input_ids': tensor([    0,  2895,  8369,  5864,    15, 38132, 15721,    36,   229,  8288,
         4400,  1076,     4,  2156,  3788,  4839,     8, 37280, 15721,    36,
         5643,  4400,  1076,     4,  2156,  6708,  4839,     7,   694, 14697,
          227, 47041,     8, 46195,  4502,  2156,    14,    32,   172, 33493,
         2500,     5,   595,  4327,  2156,    25,  2343,    30,     5,  1743,
         5468,    11, 46195,   774,  6348, 14139,  9150,    36,  1653,   241,
         5079,     8,  1127,  9343,  2156,  4482, 25606,  1653,   241,  5079,
            8,  1127,  9343,  2156,  4013,  4839,     8,    67,    36,   272,
         9683, 13184,     8, 16816,  2001, 18708,  2156,  5241, 25606,  2869,
          625,  4134,  4400,  1076,     4,  2156,  4013, 25606, 13496,     8,
          256,  4001,   337,  1755,   102,  2156,  4013,  4839,   479,     2,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>Most approaches rely on VerbNet ( Kipper et al., 2000 ) and FrameNet ( Baker et al., 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez, 2004 ; Carreras and Marquez, 2005 ) and also ( Gildea and Jurafsky, 2002 ; Pradhan et al., 2005 ; Shi and Mihalcea, 2005 ).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 16:37:07 - INFO - __main__ - Sample 827 of the training set: {'labels': tensor(5), 'input_ids': tensor([    0,  2895,  8369,  5864,    15, 38132, 15721,    36,   229,  8288,
         4400,  1076,     4,  2156,  3788,  4839,     8, 37280, 15721,    36,
         5643,  4400,  1076,     4,  2156,  6708,  4839,     7,   694, 14697,
          227, 47041,     8, 46195,  4502,  2156,    14,    32,   172, 33493,
         2500,     5,   595,  4327,  2156,    25,  2343,    30,     5,  1743,
         5468,    11, 46195,   774,  6348, 14139,  9150,    36,  1653,   241,
         5079,     8,  1127,  9343,  2156,  4482, 25606,  1653,   241,  5079,
            8,  1127,  9343,  2156,  4013,  4839,     8,    67,    36,   272,
         9683, 13184,     8, 16816,  2001, 18708,  2156,  5241, 25606,  2869,
          625,  4134,  4400,  1076,     4,  2156,  4013, 25606, 13496,     8,
          256,  4001,   337,  1755,   102,  2156,  4013,  4839,   479,     2,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>Most approaches rely on VerbNet ( Kipper et al., 2000 ) and FrameNet ( Baker et al., 1998 ) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions ( Carreras and Marquez, 2004 ; Carreras and Marquez, 2005 ) and also ( Gildea and Jurafsky, 2002 ; Pradhan et al., 2005 ; Shi and Mihalcea, 2005 ).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 16:37:07 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 16:37:15 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 16:37:15 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/acl_unsup_roberta/,  Dataset name = aclarc_sup, seed = 2021
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): LoRARobertaClassificationHead(
      (dense): PretrainingMultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=768, bias=True)
          (1): Linear(in_features=768, out_features=768, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): MultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=6, bias=True)
          (1): Linear(in_features=768, out_features=6, bias=True)
        )
      )
    )
  )
  (sigmoid): Sigmoid()
  (mse_loss): MSELoss()
  (cos): CosineSimilarity()
  (tanh): Tanh()
  (softmax): Softmax(dim=1)
  (kd_loss): DistillKL()
  (dropout): Dropout(p=0.1, inplace=False)
  (contrast): MyContrastive(
    (bce): BCEWithLogitsLoss()
    (ce): CrossEntropyLoss()
    (sup_con): SupConLoss()
  )
)
summary_path: .//seq0/640000samples/lora_init/acl_unsup_roberta/../lora_piggyback/aclarc_sup_finetune_summary

 13%|████████████████████▎                                                                                                                                            | 12/95 [00:03<00:10,  7.73it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:09<00:00,  9.72it/s]
  1%|█▋                                                                                                                                                                | 1/95 [00:01<02:52,  1.84s/it]
train acc = 0.5480, training loss = 0.0895
Epoch 1 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 85/95 [00:07<00:00, 13.77it/s]
train acc = 0.5520, training loss = 0.0790
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 10.87it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 77/95 [00:07<00:01, 13.77it/s]
train acc = 0.5809, training loss = 0.0740
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.02it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 71/95 [00:06<00:01, 13.76it/s]
train acc = 0.6849, training loss = 0.0575
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.15it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight




 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 93/95 [00:08<00:00, 13.75it/s]
train acc = 0.7507, training loss = 0.0466
Epoch 5 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.25it/s]



 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 87/95 [00:07<00:00, 13.75it/s]
train acc = 0.8072, training loss = 0.0370
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.24it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 79/95 [00:07<00:01, 13.76it/s]
train acc = 0.8566, training loss = 0.0278
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.15it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 75/95 [00:06<00:01, 13.76it/s]
train acc = 0.9039, training loss = 0.0196
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.36it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 71/95 [00:06<00:01, 13.77it/s]
train acc = 0.9349, training loss = 0.0134
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.27it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight




 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 91/95 [00:08<00:00, 13.77it/s]
train acc = 0.9579, training loss = 0.0087
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.17it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                    | 83/95 [00:07<00:00, 13.76it/s]
train acc = 0.9730, training loss = 0.0064
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 10.87it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 77/95 [00:07<00:01, 13.76it/s]
train acc = 0.9743, training loss = 0.0057
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.21it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                     | 73/95 [00:06<00:01, 13.77it/s]
train acc = 0.9836, training loss = 0.0041
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.45it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight




100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 13.78it/s]
train acc = 0.9822, training loss = 0.0036
Epoch 14 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.21it/s]



 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊          | 89/95 [00:07<00:00, 13.78it/s]
train acc = 0.9829, training loss = 0.0032
Epoch 15 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.27it/s]



 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 85/95 [00:07<00:00, 13.76it/s]
train acc = 0.9849, training loss = 0.0030
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.30it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 77/95 [00:07<00:01, 13.77it/s]
train acc = 0.9829, training loss = 0.0030
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.00it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 69/95 [00:06<00:01, 13.76it/s]
train acc = 0.9849, training loss = 0.0021
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.08it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight




 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 91/95 [00:08<00:00, 13.78it/s]
train acc = 0.9836, training loss = 0.0026
Epoch 19 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.23it/s]



 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                    | 83/95 [00:07<00:00, 13.77it/s]
train acc = 0.9868, training loss = 0.0021
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.02it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight




 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 93/95 [00:08<00:00, 13.78it/s]
train acc = 0.9842, training loss = 0.0022
Epoch 21 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 10.71it/s]



 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 85/95 [00:07<00:00, 13.75it/s]
train acc = 0.9849, training loss = 0.0020
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.14it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 81/95 [00:07<00:01, 13.77it/s]
train acc = 0.9875, training loss = 0.0020
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.30it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                     | 73/95 [00:06<00:01, 13.77it/s]
train acc = 0.9868, training loss = 0.0016
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.06it/s]

 14%|██████████████████████                                                                                                                                           | 13/95 [00:02<00:08, 10.02it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight


 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 69/95 [00:06<00:01, 13.77it/s]
train acc = 0.9888, training loss = 0.0016
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.36it/s]

  9%|███████████████▎                                                                                                                                                  | 9/95 [00:02<00:11,  7.20it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 91/95 [00:08<00:00, 13.78it/s]
train acc = 0.9836, training loss = 0.0020
Epoch 26 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.28it/s]



 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊          | 89/95 [00:07<00:00, 13.77it/s]
train acc = 0.9888, training loss = 0.0015
Epoch 27 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.61it/s]



 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 85/95 [00:07<00:00, 13.77it/s]
train acc = 0.9875, training loss = 0.0017
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.23it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                           | 79/95 [00:07<00:01, 13.76it/s]
train acc = 0.9875, training loss = 0.0014
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.24it/s]
  0%|                                                                                                                                                                          | 0/95 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.0.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.1.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.2.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.3.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.4.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.5.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.6.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.7.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.8.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.9.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.10.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.1
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.1
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.1
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.1
n,p：  model.roberta.encoder.layer.11.output.dense.masks.1
n,p：  model.classifier.dense.classifiers.1.weight
n,p：  model.classifier.dense.classifiers.1.bias
n,p：  model.classifier.out_proj.classifiers.1.weight



 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 75/95 [00:06<00:01, 13.76it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 95/95 [00:08<00:00, 11.38it/s]
  0%|                                                                                                                                                                          | 0/27 [00:00<?, ?it/s]
Path of progressive f1 score: .//seq0/640000samples/lora_init/acl_unsup_roberta//../lora_piggyback/progressive_f1_2021
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 27/27 [00:02<00:00, 11.50it/s]
07/04/2024 16:41:33 - INFO - approaches.finetune - .//seq0/640000samples/lora_init/acl_unsup_roberta/ On aclarc_sup, last epoch macro_f1 = 0.6791, acc = 0.7221 (seed=2021)