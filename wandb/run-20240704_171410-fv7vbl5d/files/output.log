[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 17:14:16 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 17:14:16 - INFO - __main__ - ==> Preparing data..
 33%|██████████████████████████████████████████████████████▎                                                                                                            | 1/3 [00:00<00:00,  2.17ba/s]
total_num:  3219
len(new_data['test']['labels']):  1429
Dataset: scierc_sup
Size of training set: 2260
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.87ba/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.57ba/s]
Sample 871 of the training set: {'labels': tensor(4), 'input_ids': tensor([    0, 42395,  2156,    52,  1455,    10, 48395, 20677,   467, 27779,
          742,    14, 41913, 43083,    11, 48188,   542, 37250,  1070,  2788,
         8488,    30,   634,    10,  1950,  8408, 13931,     9,  1198, 39221,
        22744,    14, 23684,  2857,     5, 12769, 47760,   609,   479,     2,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>Furthermore, we present a [[ standalone system ]] that resolves pronouns in << unannotated text >> by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 17:14:20 - INFO - __main__ - Sample 871 of the training set: {'labels': tensor(4), 'input_ids': tensor([    0, 42395,  2156,    52,  1455,    10, 48395, 20677,   467, 27779,
          742,    14, 41913, 43083,    11, 48188,   542, 37250,  1070,  2788,
         8488,    30,   634,    10,  1950,  8408, 13931,     9,  1198, 39221,
        22744,    14, 23684,  2857,     5, 12769, 47760,   609,   479,     2,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>Furthermore, we present a [[ standalone system ]] that resolves pronouns in << unannotated text >> by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 17:14:20 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 17:14:27 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 17:14:27 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/ai_unsup_roberta/,  Dataset name = scierc_sup, seed = 111
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )

  2%|███▍                                                                                                                                                             | 3/142 [00:02<01:33,  1.49it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight
n,p：  model.classifier.out_proj.classifiers.2.bias
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): LoRARobertaClassificationHead(
      (dense): PretrainingMultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=768, bias=True)
          (1): Linear(in_features=768, out_features=768, bias=True)
          (2): Linear(in_features=768, out_features=768, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): MultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=7, bias=True)
          (1): Linear(in_features=768, out_features=7, bias=True)
          (2): Linear(in_features=768, out_features=7, bias=True)
        )
      )
    )
  )
  (sigmoid): Sigmoid()
  (mse_loss): MSELoss()
  (cos): CosineSimilarity()
  (tanh): Tanh()
  (softmax): Softmax(dim=1)
  (kd_loss): DistillKL()
  (dropout): Dropout(p=0.1, inplace=False)
  (contrast): MyContrastive(
    (bce): BCEWithLogitsLoss()
    (ce): CrossEntropyLoss()
    (sup_con): SupConLoss()
  )
)
summary_path: .//seq0/640000samples/lora_init/ai_unsup_roberta/../lora_piggyback/scierc_sup_finetune_summary





100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.21it/s]




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.89it/s]

n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.85it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.7695, training loss = 0.0463

  5%|███████▉                                                                                                                                                         | 7/142 [00:02<00:25,  5.25it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.87it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.8792, training loss = 0.0270

  5%|███████▉                                                                                                                                                         | 7/142 [00:02<00:26,  5.17it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.83it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9168, training loss = 0.0180

  8%|████████████▍                                                                                                                                                   | 11/142 [00:02<00:15,  8.72it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.05it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9491, training loss = 0.0112

 12%|███████████████████▏                                                                                                                                            | 17/142 [00:02<00:10, 11.79it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.11it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9619, training loss = 0.0087
Epoch 7 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.11it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9788, training loss = 0.0050
Epoch 8 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 137/142 [00:11<00:00, 13.78it/s]
train acc = 0.9801, training loss = 0.0041
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.04it/s]
  1%|█▏                                                                                                                                                               | 1/142 [00:01<03:49,  1.63s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋            | 131/142 [00:11<00:00, 13.76it/s]
train acc = 0.9881, training loss = 0.0028
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.93it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉          | 133/142 [00:11<00:00, 13.78it/s]
train acc = 0.9903, training loss = 0.0021
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.82it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉          | 133/142 [00:11<00:00, 13.78it/s]
train acc = 0.9912, training loss = 0.0019
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.85it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 137/142 [00:11<00:00, 13.77it/s]
train acc = 0.9951, training loss = 0.0015
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.03it/s]
  2%|███▍                                                                                                                                                             | 3/142 [00:01<01:02,  2.21it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 141/142 [00:11<00:00, 13.78it/s]
train acc = 0.9960, training loss = 0.0009
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.00it/s]
  1%|█▏                                                                                                                                                               | 1/142 [00:01<04:12,  1.79s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 141/142 [00:11<00:00, 13.76it/s]
train acc = 0.9973, training loss = 0.0010
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.74it/s]
  2%|███▍                                                                                                                                                             | 3/142 [00:01<01:07,  2.05it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.87it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9987, training loss = 0.0005

  4%|█████▋                                                                                                                                                           | 5/142 [00:02<00:38,  3.57it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.82it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9978, training loss = 0.0006

  1%|█▏                                                                                                                                                               | 1/142 [00:01<04:40,  1.99s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 139/142 [00:12<00:00, 13.75it/s]
train acc = 0.9996, training loss = 0.0003
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.55it/s]
  2%|███▍                                                                                                                                                             | 3/142 [00:01<01:05,  2.12it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.92it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9991, training loss = 0.0003

  4%|█████▋                                                                                                                                                           | 5/142 [00:01<00:38,  3.59it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.85it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9991, training loss = 0.0003

  4%|█████▋                                                                                                                                                           | 5/142 [00:02<00:40,  3.42it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 135/142 [00:11<00:00, 13.78it/s]
train acc = 0.9996, training loss = 0.0002
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.74it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 135/142 [00:11<00:00, 13.77it/s]
train acc = 0.9969, training loss = 0.0007
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.77it/s]
  2%|███▍                                                                                                                                                             | 3/142 [00:01<00:57,  2.44it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 141/142 [00:11<00:00, 13.77it/s]
train acc = 0.9996, training loss = 0.0002
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.16it/s]
  2%|███▍                                                                                                                                                             | 3/142 [00:01<01:09,  1.99it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.81it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 1.0000, training loss = 0.0001

  2%|███▍                                                                                                                                                             | 3/142 [00:01<01:13,  1.89it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 141/142 [00:12<00:00, 13.78it/s]
train acc = 1.0000, training loss = 0.0001
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.71it/s]
  4%|█████▋                                                                                                                                                           | 5/142 [00:01<00:34,  3.92it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.87it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 0.9996, training loss = 0.0001

  5%|███████▉                                                                                                                                                         | 7/142 [00:02<00:25,  5.39it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.94it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 1.0000, training loss = 0.0001

  5%|███████▉                                                                                                                                                         | 7/142 [00:02<00:26,  5.16it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 11.84it/s]
  0%|                                                                                                                                                                         | 0/142 [00:00<?, ?it/s]
train acc = 1.0000, training loss = 0.0001

  2%|███▍                                                                                                                                                             | 3/142 [00:02<01:23,  1.67it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight





 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 141/142 [00:12<00:00, 13.78it/s]
train acc = 0.9996, training loss = 0.0001
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:12<00:00, 11.45it/s]
  5%|███████▉                                                                                                                                                         | 7/142 [00:01<00:24,  5.55it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.0.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.1.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.2.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.3.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.4.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.5.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.6.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.7.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.8.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.9.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.10.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.2
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.2
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.2
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.2
n,p：  model.roberta.encoder.layer.11.output.dense.masks.2
n,p：  model.classifier.dense.classifiers.2.weight
n,p：  model.classifier.dense.classifiers.2.bias
n,p：  model.classifier.out_proj.classifiers.2.weight




100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 142/142 [00:11<00:00, 12.01it/s]
  0%|                                                                                                                                                                         | 0/150 [00:00<?, ?it/s]


100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:05<00:00, 25.36it/s]
07/04/2024 17:20:33 - INFO - approaches.finetune - .//seq0/640000samples/lora_init/ai_unsup_roberta/ On scierc_sup, last epoch macro_f1 = 0.7152, acc = 0.7735 (seed=111)
Path of progressive f1 score: .//seq0/640000samples/lora_init/ai_unsup_roberta//../lora_piggyback/progressive_f1_111
Path of progressive accuracy: .//seq0/640000samples/lora_init/ai_unsup_roberta//../lora_piggyback/progressive_acc_111