[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 17:54:59 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 17:54:59 - INFO - __main__ - ==> Preparing data..
 12%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                                              | 1/8 [00:00<00:03,  1.85ba/s]
total_num:  4169
len(new_data['test']['labels']):  5896
Dataset: chemprot_sup
Size of training set: 2667


100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03<00:00,  2.21ba/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.54ba/s]
Sample 441 of the training set: {'labels': tensor(1), 'input_ids': tensor([    0,   170,   303,    14,  4878,     9,     5, 30708, 23982, 13575,
        32027,  5112,  3141,     9,     5, 48188, 42193,   524,   833,  8488,
          204,    12, 13941,  2413,  1588,  2457,  4360,    36,   306,    12,
         4546,   510,   131,   303,    11, 16596,  4603,    43,     8,     5,
        39872, 30321,  3998,   636,   524,   833,   132,    12,   424,  1696,
           12,   134,    12, 46988,    12,   401,    12, 37321,   219, 10839,
          808, 28765,   646,   306,     6,   245,    12,   428,   742,   181,
         4503, 40314,    36, 17297,  3808,   131,   303,    11, 20346, 26627,
           43,    21,  5329, 23836, 16317,    30,    10, 45238,   467,  8200,
          129,  1050,   741,   245,   500,     8, 48395, 44193,   741,   245,
        27779,  8174,     2,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>We found that reduction of the carcinogenic hydroxylamines of the << aromatic amine >> 4-aminobiphenyl (4-ABP; found in cigarette smoke) and the heterocyclic amine 2-amino-1-methyl-6-phenylimidazo [4,5-b] pyridine (PhIP; found in grilled meats) was indeed catalyzed by a purified system containing only human b5R and [[ cyt b5 ]].</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 17:55:06 - INFO - __main__ - Sample 441 of the training set: {'labels': tensor(1), 'input_ids': tensor([    0,   170,   303,    14,  4878,     9,     5, 30708, 23982, 13575,
        32027,  5112,  3141,     9,     5, 48188, 42193,   524,   833,  8488,
          204,    12, 13941,  2413,  1588,  2457,  4360,    36,   306,    12,
         4546,   510,   131,   303,    11, 16596,  4603,    43,     8,     5,
        39872, 30321,  3998,   636,   524,   833,   132,    12,   424,  1696,
           12,   134,    12, 46988,    12,   401,    12, 37321,   219, 10839,
          808, 28765,   646,   306,     6,   245,    12,   428,   742,   181,
         4503, 40314,    36, 17297,  3808,   131,   303,    11, 20346, 26627,
           43,    21,  5329, 23836, 16317,    30,    10, 45238,   467,  8200,
          129,  1050,   741,   245,   500,     8, 48395, 44193,   741,   245,
        27779,  8174,     2,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>We found that reduction of the carcinogenic hydroxylamines of the << aromatic amine >> 4-aminobiphenyl (4-ABP; found in cigarette smoke) and the heterocyclic amine 2-amino-1-methyl-6-phenylimidazo [4,5-b] pyridine (PhIP; found in grilled meats) was indeed catalyzed by a purified system containing only human b5R and [[ cyt b5 ]].</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 17:55:06 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 17:55:16 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 17:55:16 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/pubmed_unsup_roberta/,  Dataset name = chemprot_sup, seed = 222
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(

  3%|‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                            | 5/167 [00:03<01:07,  2.39it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight
n,pÔºö  model.classifier.out_proj.classifiers.4.bias
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): LoRARobertaClassificationHead(
      (dense): PretrainingMultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=768, bias=True)
          (1): Linear(in_features=768, out_features=768, bias=True)
          (2): Linear(in_features=768, out_features=768, bias=True)
          (3): Linear(in_features=768, out_features=768, bias=True)
          (4): Linear(in_features=768, out_features=768, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): MultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=13, bias=True)
          (1): Linear(in_features=768, out_features=13, bias=True)
          (2): Linear(in_features=768, out_features=13, bias=True)
          (3): Linear(in_features=768, out_features=13, bias=True)
          (4): Linear(in_features=768, out_features=13, bias=True)
        )
      )
    )
  )
  (sigmoid): Sigmoid()
  (mse_loss): MSELoss()
  (cos): CosineSimilarity()
  (tanh): Tanh()
  (softmax): Softmax(dim=1)
  (kd_loss): DistillKL()
  (dropout): Dropout(p=0.1, inplace=False)
  (contrast): MyContrastive(
    (bce): BCEWithLogitsLoss()
    (ce): CrossEntropyLoss()
    (sup_con): SupConLoss()
  )
)
summary_path: .//seq0/640000samples/lora_init/pubmed_unsup_roberta/../lora_piggyback/chemprot_sup_finetune_summary





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.15it/s]






100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.82it/s]

n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 14.13it/s]
train acc = 0.8279, training loss = 0.0313
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.65it/s]
  3%|‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                            | 5/167 [00:01<00:44,  3.65it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.10it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.8980, training loss = 0.0194

  3%|‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                            | 5/167 [00:02<00:47,  3.41it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 165/167 [00:13<00:00, 13.77it/s]
train acc = 0.9359, training loss = 0.0124
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 11.98it/s]
  3%|‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                            | 5/167 [00:01<00:41,  3.86it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.20it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9580, training loss = 0.0082

  3%|‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                                                            | 5/167 [00:02<00:50,  3.24it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.86it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9764, training loss = 0.0053

  5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                                                        | 9/167 [00:02<00:22,  7.04it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.17it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9689, training loss = 0.0053

  4%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                                                          | 7/167 [00:02<00:35,  4.53it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.73it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9813, training loss = 0.0037

  5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                                                        | 9/167 [00:02<00:23,  6.64it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.01it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9805, training loss = 0.0037

  5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                                                        | 9/167 [00:02<00:25,  6.21it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.82it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9831, training loss = 0.0032

  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                                                     | 11/167 [00:02<00:19,  8.10it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 11.99it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9846, training loss = 0.0028

  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                                                   | 13/167 [00:02<00:16,  9.30it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 11.96it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9891, training loss = 0.0024

  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                                                     | 11/167 [00:02<00:20,  7.66it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.79it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9888, training loss = 0.0022

  4%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                                                          | 7/167 [00:02<00:31,  5.10it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.01it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9891, training loss = 0.0017

  7%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                                                     | 11/167 [00:02<00:18,  8.36it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.11it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9895, training loss = 0.0017
Epoch 16 started
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.00it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9910, training loss = 0.0017
Epoch 17 started
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 155/167 [00:12<00:00, 13.76it/s]
train acc = 0.9933, training loss = 0.0011
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.04it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 159/167 [00:13<00:00, 13.77it/s]
train acc = 0.9903, training loss = 0.0017
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.11it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 163/167 [00:13<00:00, 13.75it/s]
train acc = 0.9873, training loss = 0.0020
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.06it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 163/167 [00:13<00:00, 13.76it/s]
train acc = 0.9891, training loss = 0.0016
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.84it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 163/167 [00:13<00:00, 13.76it/s]
train acc = 0.9918, training loss = 0.0012
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 11.95it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 157/167 [00:13<00:00, 13.76it/s]
train acc = 0.9925, training loss = 0.0010
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.83it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 157/167 [00:13<00:00, 13.76it/s]
train acc = 0.9921, training loss = 0.0009
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.88it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 161/167 [00:13<00:00, 13.77it/s]
train acc = 0.9918, training loss = 0.0011
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.14it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight






 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 163/167 [00:13<00:00, 13.76it/s]
train acc = 0.9936, training loss = 0.0009
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.92it/s]
  2%|‚ñà‚ñà‚ñâ                                                                                                                                                              | 3/167 [00:01<01:11,  2.29it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.28it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9933, training loss = 0.0008

  2%|‚ñà‚ñà‚ñâ                                                                                                                                                              | 3/167 [00:02<01:27,  1.86it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:14<00:00, 11.92it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9948, training loss = 0.0007

  5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                                                                                                                                        | 9/167 [00:02<00:22,  6.96it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.13it/s]
  0%|                                                                                                                                                                         | 0/167 [00:00<?, ?it/s]
train acc = 0.9948, training loss = 0.0007

  8%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                                                   | 13/167 [00:02<00:15,  9.84it/s]
n,pÔºö  model.roberta.encoder.layer.0.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.0.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.0.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.1.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.1.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.2.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.2.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.3.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.3.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.4.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.4.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.5.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.5.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.6.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.6.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.7.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.7.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.8.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.8.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.9.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.9.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.10.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.10.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.query.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.key.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.self.value.masks.4
n,pÔºö  model.roberta.encoder.layer.11.attention.output.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.intermediate.dense.masks.4
n,pÔºö  model.roberta.encoder.layer.11.output.dense.masks.4
n,pÔºö  model.classifier.dense.classifiers.4.weight
n,pÔºö  model.classifier.dense.classifiers.4.bias
n,pÔºö  model.classifier.out_proj.classifiers.4.weight





100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [00:13<00:00, 12.19it/s]
  0%|                                                                                                                                                                         | 0/463 [00:00<?, ?it/s]







 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 452/463 [00:14<00:00, 35.51it/s]
Path of progressive f1 score: .//seq0/640000samples/lora_init/pubmed_unsup_roberta//../lora_piggyback/progressive_f1_222
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 463/463 [00:14<00:00, 31.09it/s]
07/04/2024 18:02:30 - INFO - approaches.finetune - .//seq0/640000samples/lora_init/pubmed_unsup_roberta/ On chemprot_sup, last epoch macro_f1 = 0.7426, acc = 0.7426 (seed=222)