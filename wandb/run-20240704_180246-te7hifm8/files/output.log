[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 18:02:50 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 18:02:50 - INFO - __main__ - ==> Preparing data..
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.96ba/s]
total_num:  286
len(new_data['test']['labels']):  570
Dataset: camera_sup
Size of training set: 230
Size of testing set: 626
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.57ba/s]
Sample 199 of the training set: {'labels': tensor(1), 'input_ids': tensor([    0,  4301,  1437,     2, 37615,  2016,    13,   477,     8,  4511,
           53,    10,   372,  2280,    13,  4126, 15501,   479,  1437,     2,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>weight </s>rather heavy for point and shoot but a great camera for semi pros. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 18:02:53 - INFO - __main__ - Sample 199 of the training set: {'labels': tensor(1), 'input_ids': tensor([    0,  4301,  1437,     2, 37615,  2016,    13,   477,     8,  4511,
           53,    10,   372,  2280,    13,  4126, 15501,   479,  1437,     2,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>weight </s>rather heavy for point and shoot but a great camera for semi pros. </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 18:02:53 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 18:03:03 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 18:03:03 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/camera_unsup_roberta/,  Dataset name = camera_sup, seed = 222
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 8x768]
                      (1): Parameter containing: [torch.FloatTensor of size 8x768]
                      (2): Parameter containing: [torch.FloatTensor of size 8x768]
                      (3): Parameter containing: [torch.FloatTensor of size 8x768]
                      (4): Parameter containing: [torch.FloatTensor of size 8x768]
                      (5): Parameter containing: [torch.FloatTensor of size 8x768]
                  )
                  (lora_Bs): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x8]
                      (1): Parameter containing: [torch.FloatTensor of size 768x8]
                      (2): Parameter containing: [torch.FloatTensor of size 768x8]
                      (3): Parameter containing: [torch.FloatTensor of size 768x8]
                      (4): Parameter containing: [torch.FloatTensor of size 768x8]
                      (5): Parameter containing: [torch.FloatTensor of size 768x8]
                  )
                  (masks): ParameterDict(
                      (0): Parameter containing: [torch.FloatTensor of size 768x768]
                      (1): Parameter containing: [torch.FloatTensor of size 768x768]
                      (2): Parameter containing: [torch.FloatTensor of size 768x768]
                      (3): Parameter containing: [torch.FloatTensor of size 768x768]
                      (4): Parameter containing: [torch.FloatTensor of size 768x768]
                      (5): Parameter containing: [torch.FloatTensor of size 768x768]
                  )
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x768]
                    (1): Parameter containing: [torch.FloatTensor of size 8x768]
                    (2): Parameter containing: [torch.FloatTensor of size 8x768]
                    (3): Parameter containing: [torch.FloatTensor of size 8x768]
                    (4): Parameter containing: [torch.FloatTensor of size 8x768]
                    (5): Parameter containing: [torch.FloatTensor of size 8x768]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x8]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (1): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (2): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (3): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (4): Parameter containing: [torch.FloatTensor of size 3072x768]
                    (5): Parameter containing: [torch.FloatTensor of size 3072x768]
                )
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 8x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 8x3072]
                )
                (lora_Bs): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x8]
                    (1): Parameter containing: [torch.FloatTensor of size 768x8]
                    (2): Parameter containing: [torch.FloatTensor of size 768x8]
                    (3): Parameter containing: [torch.FloatTensor of size 768x8]
                    (4): Parameter containing: [torch.FloatTensor of size 768x8]
                    (5): Parameter containing: [torch.FloatTensor of size 768x8]
                )
                (masks): ParameterDict(
                    (0): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (1): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (2): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (3): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (4): Parameter containing: [torch.FloatTensor of size 768x3072]
                    (5): Parameter containing: [torch.FloatTensor of size 768x3072]
                )
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  9.69it/s]
train acc = 0.5478, training loss = 0.0449
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  4.00it/s]
 33%|██████████████████████████████████████████████████████                                                                                                            | 5/15 [00:01<00:02,  3.64it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0354
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.41it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.53it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0305

 60%|█████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                | 9/15 [00:02<00:00,  6.55it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0281
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.15it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0277
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  4.96it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5

 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 11/15 [00:02<00:00,  8.13it/s]
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0276
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.28it/s]
  7%|██████████▊                                                                                                                                                       | 1/15 [00:01<00:23,  1.71s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0272
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.38it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.56it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0282

 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 11/15 [00:02<00:00,  8.66it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0273
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.72it/s]
  7%|██████████▊                                                                                                                                                       | 1/15 [00:01<00:22,  1.63s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0253
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.53it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.53it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0269

 47%|███████████████████████████████████████████████████████████████████████████▌                                                                                      | 7/15 [00:02<00:01,  5.15it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0269
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.35it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  4.97it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0263
Epoch 13 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5

n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0267
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.44it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.04it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0269
Epoch 15 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.66it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0258

 47%|███████████████████████████████████████████████████████████████████████████▌                                                                                      | 7/15 [00:02<00:01,  5.27it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0267
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.46it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.62it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0243
Epoch 18 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.67it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.8565, training loss = 0.0246

 33%|██████████████████████████████████████████████████████                                                                                                            | 5/15 [00:01<00:02,  3.68it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0225
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.50it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8565, training loss = 0.0215
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.89it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00, 11.02it/s]
train acc = 0.8565, training loss = 0.0147
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.25it/s]
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                | 9/15 [00:01<00:00,  7.69it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.8652, training loss = 0.0117
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  6.24it/s]
  7%|██████████▊                                                                                                                                                       | 1/15 [00:01<00:20,  1.49s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.9130, training loss = 0.0101
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.82it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.57it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
train acc = 0.9565, training loss = 0.0089
Epoch 25 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight

 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 11/15 [00:02<00:00,  8.59it/s]
train acc = 0.9696, training loss = 0.0081
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.68it/s]
 20%|████████████████████████████████▍                                                                                                                                 | 3/15 [00:01<00:05,  2.20it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.9609, training loss = 0.0069
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.71it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00, 11.00it/s]
train acc = 0.9696, training loss = 0.0074
Epoch 28 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.21it/s]
 47%|███████████████████████████████████████████████████████████████████████████▌                                                                                      | 7/15 [00:02<00:01,  5.27it/s]
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
train acc = 0.9783, training loss = 0.0057
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.51it/s]
  0%|                                                                                                                                                                          | 0/15 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.0.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.1.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.2.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.3.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.4.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.5.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.6.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.7.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.8.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.9.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.10.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.5
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.5
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.5
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.5
n,p：  model.roberta.encoder.layer.11.output.dense.masks.5
n,p：  model.classifier.dense.classifiers.5.weight
n,p：  model.classifier.dense.classifiers.5.bias
n,p：  model.classifier.out_proj.classifiers.5.weight
n,p：  model.classifier.out_proj.classifiers.5.bias
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:02<00:00,  5.25it/s]
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 15.18it/s]
07/04/2024 18:04:29 - INFO - approaches.finetune - .//seq0/640000samples/lora_init/camera_unsup_roberta/ On camera_sup, last epoch macro_f1 = 0.8890, acc = 0.9233 (seed=222)
Path of progressive f1 score: .//seq0/640000samples/lora_init/camera_unsup_roberta//../lora_piggyback/progressive_f1_222
Path of progressive accuracy: .//seq0/640000samples/lora_init/camera_unsup_roberta//../lora_piggyback/progressive_acc_222
Final f1 score: .//seq0/640000samples/lora_init/camera_unsup_roberta//../lora_piggyback/f1_222
Final accuracy: .//seq0/640000samples/lora_init/camera_unsup_roberta//../lora_piggyback/acc_222