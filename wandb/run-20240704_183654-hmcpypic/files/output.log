[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
07/04/2024 18:36:59 - INFO - __main__ - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
==> Preparing data..
07/04/2024 18:36:59 - INFO - __main__ - ==> Preparing data..
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  5.12ba/s]
Dataset: restaurant_sup
Size of training set: 3452
Size of testing set: 1120
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.82ba/s]
Sample 1265 of the training set: {'labels': tensor(0), 'input_ids': tensor([    0,  2977,   415,  7134,  1437,     2, 38540,    21,    10,   410,
        18698,    53,   202, 10964,    19, 24791,  7134,     8,   181, 13706,
        16597,    36,   338,  4917,  1725,  1020,    21,   761,     9, 10513,
          600,   322,     2,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>goat cheese </s>Mine was a little burnt but still delicious with goat cheese and panchetta (raddichio was kind of bitter though).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
07/04/2024 18:37:03 - INFO - __main__ - Sample 1265 of the training set: {'labels': tensor(0), 'input_ids': tensor([    0,  2977,   415,  7134,  1437,     2, 38540,    21,    10,   410,
        18698,    53,   202, 10964,    19, 24791,  7134,     8,   181, 13706,
        16597,    36,   338,  4917,  1725,  1020,    21,   761,     9, 10513,
          600,   322,     2,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,
            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}. Decode to: <s>goat cheese </s>Mine was a little burnt but still delicious with goat cheese and panchetta (raddichio was kind of bitter though).</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
==> Building model..
07/04/2024 18:37:03 - INFO - __main__ - ==> Building model..
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
MyModel(
  (model): LoRARobertaForSequenceClassification(
    (roberta): LoRARobertaModel(
      (embeddings): LoRARobertaEmbeddings(
        (word_embeddings): Embedding(50265, 768, padding_idx=1)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): LoRARobertaEncoder(
        (layer): ModuleList(
          (0): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): LoRARobertaLayer(
            (attention): LoRARobertaAttention(
              (self): LoRARobertaSelfAttention(
                (query): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (key): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (value): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): LoRARobertaSelfOutput(
                (dense): LoRAPiggybackLinear(
                  (lora_dropout): Identity()
                  (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                  (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                  (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x768])
                )
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): LoRARobertaIntermediate(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x768])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 3072x768])
              )
              (intermediate_act_fn): GELU()
            )
            (output): LoRARobertaOutput(
              (dense): LoRAPiggybackLinear(
                (lora_dropout): Identity()
                (lora_As): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 8x3072])
                (lora_Bs): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x8])
                (masks): ParameterDict(  (0): Parameter containing: [torch.FloatTensor of size 768x3072])
              )
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): LoRARobertaClassificationHead(
      (dense): PretrainingMultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=768, bias=True)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): MultiTaskClassifier(
        (classifiers): ModuleDict(
          (0): Linear(in_features=768, out_features=3, bias=True)
        )
      )
    )
  )
  (sigmoid): Sigmoid()
  (mse_loss): MSELoss()
  (cos): CosineSimilarity()
  (tanh): Tanh()
  (softmax): Softmax(dim=1)
  (kd_loss): DistillKL()
  (dropout): Dropout(p=0.1, inplace=False)
  (contrast): MyContrastive(
    (bce): BCEWithLogitsLoss()
    (ce): CrossEntropyLoss()
    (sup_con): SupConLoss()
  )
)
summary_path: .//seq0/640000samples/lora_init/restaurant_unsup_roberta/../lora_piggyback/restaurant_sup_finetune_summary
Epoch 0 started
/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
07/04/2024 18:37:09 - INFO - approaches.finetune - ***** Running training *****
07/04/2024 18:37:09 - INFO - approaches.finetune - Pretrained Model = .//seq0/640000samples/lora_init/restaurant_unsup_roberta/,  Dataset name = restaurant_sup, seed = 444
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 211/216 [00:17<00:00, 13.78it/s]
train acc = 0.6049, training loss = 0.0584
Epoch 1 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:18<00:00, 11.80it/s]







 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 195/216 [00:15<00:01, 13.75it/s]
train acc = 0.7685, training loss = 0.0360
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.47it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 205/216 [00:16<00:00, 13.78it/s]
train acc = 0.8250, training loss = 0.0276
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.46it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                   | 189/216 [00:15<00:01, 13.77it/s]
train acc = 0.8537, training loss = 0.0224
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.51it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 199/216 [00:16<00:01, 13.76it/s]
train acc = 0.8980, training loss = 0.0170
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.46it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 211/216 [00:16<00:00, 13.78it/s]
train acc = 0.9212, training loss = 0.0127
Epoch 6 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.44it/s]







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.53it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9519, training loss = 0.0084
Epoch 7 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 199/216 [00:15<00:01, 13.76it/s]
train acc = 0.9632, training loss = 0.0068
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.53it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 211/216 [00:16<00:00, 13.77it/s]
train acc = 0.9725, training loss = 0.0050
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.57it/s]
  0%|▋                                                                                                                                                                | 1/216 [00:01<05:55,  1.65s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 195/216 [00:15<00:01, 13.76it/s]
train acc = 0.9800, training loss = 0.0036
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.47it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 205/216 [00:16<00:00, 13.77it/s]
train acc = 0.9791, training loss = 0.0039
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.40it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.57it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9864, training loss = 0.0024
Epoch 12 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 201/216 [00:16<00:01, 13.76it/s]
train acc = 0.9913, training loss = 0.0018
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.50it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 211/216 [00:16<00:00, 13.77it/s]
train acc = 0.9890, training loss = 0.0018
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.44it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 191/216 [00:15<00:01, 13.77it/s]
train acc = 0.9890, training loss = 0.0019
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.65it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍         | 203/216 [00:16<00:00, 13.77it/s]
train acc = 0.9902, training loss = 0.0016
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.49it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.60it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
train acc = 0.9925, training loss = 0.0012

  0%|▋                                                                                                                                                                | 1/216 [00:01<06:43,  1.88s/it]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 195/216 [00:15<00:01, 13.77it/s]
train acc = 0.9942, training loss = 0.0010
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.29it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 205/216 [00:16<00:00, 13.78it/s]
train acc = 0.9930, training loss = 0.0012
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.38it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 213/216 [00:17<00:00, 13.77it/s]
train acc = 0.9928, training loss = 0.0013
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.39it/s]
  1%|██▏                                                                                                                                                              | 3/216 [00:01<01:42,  2.07it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 191/216 [00:15<00:01, 13.76it/s]
train acc = 0.9930, training loss = 0.0010
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.47it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 93%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 201/216 [00:16<00:01, 13.77it/s]
train acc = 0.9939, training loss = 0.0008
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.43it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 211/216 [00:16<00:00, 13.78it/s]
train acc = 0.9945, training loss = 0.0008
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.45it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight







 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 191/216 [00:15<00:01, 13.76it/s]
train acc = 0.9942, training loss = 0.0008
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.29it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 205/216 [00:16<00:00, 13.77it/s]
train acc = 0.9959, training loss = 0.0006
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.62it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 215/216 [00:17<00:00, 13.78it/s]
train acc = 0.9965, training loss = 0.0007
Epoch 26 started
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.44it/s]







 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 199/216 [00:15<00:01, 13.77it/s]
train acc = 0.9957, training loss = 0.0006
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.51it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 205/216 [00:16<00:00, 13.76it/s]
train acc = 0.9965, training loss = 0.0006
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.58it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]
n,p：  model.roberta.encoder.layer.0.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.0.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.0.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.0.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.0.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.1.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.1.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.1.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.1.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.2.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.2.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.2.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.2.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.3.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.3.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.3.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.3.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.4.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.4.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.4.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.4.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.5.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.5.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.5.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.5.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.6.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.6.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.6.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.6.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.7.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.7.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.7.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.7.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.8.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.8.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.8.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.8.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.9.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.9.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.9.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.9.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.10.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.10.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.10.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.10.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.query.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.key.masks.0
n,p：  model.roberta.encoder.layer.11.attention.self.value.masks.0
n,p：  model.roberta.encoder.layer.11.attention.output.dense.masks.0
n,p：  model.roberta.encoder.layer.11.intermediate.dense.masks.0
n,p：  model.roberta.encoder.layer.11.output.dense.masks.0
n,p：  model.classifier.dense.classifiers.0.weight
n,p：  model.classifier.dense.classifiers.0.bias
n,p：  model.classifier.out_proj.classifiers.0.weight








 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 213/216 [00:17<00:00, 13.77it/s]
train acc = 0.9951, training loss = 0.0005
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 216/216 [00:17<00:00, 12.34it/s]
  0%|                                                                                                                                                                         | 0/216 [00:00<?, ?it/s]Exception ignored in: <function _releaseLock at 0x7fb68cabb320>
Traceback (most recent call last):
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/logging/__init__.py", line 221, in _releaseLock
    def _releaseLock():
KeyboardInterrupt
Traceback (most recent call last):
  File "finetune.py", line 150, in <module>
    main()
  File "finetune.py", line 146, in main
    appr.train(model, accelerator, train_loader, test_loader)
  File "/home/0k9d0h1/piggyback/piggyback-ContinualLM/approaches/finetune.py", line 80, in train
    model, optimizer, train_loader, accelerator, lr_scheduler)
  File "/home/0k9d0h1/piggyback/piggyback-ContinualLM/approaches/finetune.py", line 164, in train_epoch
    for batch, inputs in enumerate(dataloader):
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/accelerate/data_loader.py", line 301, in __iter__
    for batch in super().__iter__():
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 368, in __iter__
    return self._get_iterator()
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 314, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 965, in __init__
    self._reset(loader, first_iter=True)
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 996, in _reset
    self._try_put_index()
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1241, in _try_put_index
    self._index_queues[worker_queue_idx].put((self._send_idx, index))
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/multiprocessing/queues.py", line 87, in put
    self._start_thread()
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/multiprocessing/queues.py", line 170, in _start_thread
    self._thread.start()
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/threading.py", line 857, in start
    self._started.wait()
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/threading.py", line 552, in wait
    signaled = self._cond.wait(timeout)
  File "/home/0k9d0h1/anaconda3/envs/DAS/lib/python3.7/threading.py", line 296, in wait
    waiter.acquire()
KeyboardInterrupt