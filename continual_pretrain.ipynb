{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Simple Example of Continue Pre-trainining\n",
        "\n",
        "This notebook's purpose is to demonstrate the implementation of the soft-masking concept (refer to the [DAG](https://arxiv.org/abs/2301.08986) and [DAS](https://openreview.net/forum?id=m_GDIItaI3o)). It is not designed to yield effective results in real-world scenarios. Its simplicity lies in the fact that:\n",
        "\n",
        "*   We avoid using advanced packages, including huggingface.\n",
        "*   We employ a basic fully connected network instead of any pre-trained language models or LSTM.\n",
        "*   The data is synthetic, and we do not implement a real tokenizer or masked language model loss\n"
      ],
      "metadata": {
        "id": "a19g2_rSwf_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocess"
      ],
      "metadata": {
        "id": "b944QHzowj3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the necessary packages"
      ],
      "metadata": {
        "id": "hb0zBYMSyjHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random, os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "PoOsMF3RyiWy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a basic tokenizer. This tokenizer's vocabulary is created from the provided corpus. It is not suitable for real-world applications, as this simplistic approach cannot manage any words that are not already in the corpus."
      ],
      "metadata": {
        "id": "62Kr5Z6J516h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(corpus):\n",
        "  # Build vocabulary\n",
        "\n",
        "  vocab = defaultdict(int)\n",
        "  idx = 0\n",
        "  for text in corpus:\n",
        "      for word in text.split():\n",
        "        if word not in vocab:\n",
        "          vocab[word] = idx\n",
        "          idx += 1\n",
        "\n",
        "  # Use vocabulary\n",
        "  tokenizerd_corpus = []\n",
        "  for text in corpus:\n",
        "      tokenized_text = []\n",
        "      for word in text.split():\n",
        "          tokenized_text.append(vocab[word])\n",
        "      tokenizerd_corpus.append(tokenized_text)\n",
        "\n",
        "  return {'idx': tokenizerd_corpus}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TMujK8vjwlH5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we implement a helper function to assist in grouping the texts in the corpus. During pre-training, we focus less on individual 'instances' and instead concatenate all instances in the corpus into a single, long text."
      ],
      "metadata": {
        "id": "p_WDgzkj6ewJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_texts(examples,max_seq_length):\n",
        "\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= max_seq_length:\n",
        "        total_length = (total_length // max_seq_length) * max_seq_length\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i: i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "\n",
        "    #Lets also give some synthetic label here for pre-training task\n",
        "    label_ids = [0,1]\n",
        "    result['labels'] = []\n",
        "    for idx in result['idx']:\n",
        "      result['labels'].append(random.sample(label_ids, 1))\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "lpWwAAdf6ieA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to create a custom PyTorch dataset, since our data is formatted as a dictionary."
      ],
      "metadata": {
        "id": "_qlKSXzj6yjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['idx'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        data_tensor = {}\n",
        "        for key, value in self.data.items():\n",
        "          data_item = self.data[key][idx]\n",
        "          data_tensor[key] = torch.tensor(data_item, dtype=torch.float)\n",
        "\n",
        "        return data_tensor\n"
      ],
      "metadata": {
        "id": "aSlUYZO06y3p"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The neural network used here is a basic fully connected network. For simplicity, we assume the pre-training task involves binary classification. It's important to note that there are two parameters associated with our soft-mask, which will be utilized later.\n",
        "\n",
        "```\n",
        "    def forward(self, x, f1_mask=None, f2_mask=None):\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BXDZbyiy7B7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NNSoftmask(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NNSoftmask, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(300, 50)\n",
        "        self.fc1 = nn.Linear(50,30)\n",
        "        self.fc2 = nn.Linear(30,10)\n",
        "        self.head = nn.Linear(10,1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.return_representation = False\n",
        "\n",
        "    def forward(self, x, f1_mask=None, f2_mask=None):\n",
        "\n",
        "        x = self.word_embeddings(x)\n",
        "        if f1_mask is None:\n",
        "          x = self.dropout(F.relu(self.fc1(x)))\n",
        "        else:\n",
        "          x = self.dropout(F.relu(self.fc1(x) * f1_mask)) # for softmask\n",
        "\n",
        "        if f2_mask is None:\n",
        "          x = self.dropout(F.relu(self.fc2(x)))\n",
        "        else:\n",
        "          x = self.dropout(F.relu(self.fc2(x) * f2_mask)) # for softmask\n",
        "        if self.return_representation:\n",
        "          return x\n",
        "        else:\n",
        "          x = self.sigmoid(self.head(x).mean(1))\n",
        "          return x"
      ],
      "metadata": {
        "id": "hGCuQYPF7CRy"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can initialize our synthetic data and the model."
      ],
      "metadata": {
        "id": "Ob7ktF5S79sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "        '''\n",
        "        Apparently Prides Osteria had a rough summer as evidenced by the almost empty dining room at 6:30 on a Friday night. However new blood in the kitchen seems to have revitalized the food from other customers recent visits. Waitstaff was warm but unobtrusive. By 8 pm or so when we left the bar was full and the dining room was much more lively than it had been. Perhaps Beverly residents prefer a later seating. After reading the mixed reviews of late I was a little tentative over our choice but luckily there was nothing to worry about in the food department. We started with the fried dough, burrata and prosciutto which were all lovely. Then although they don't offer half portions of pasta we each ordered the entree size and split them. We chose the tagliatelle bolognese and a four cheese filled pasta in a creamy sauce with bacon, asparagus and grana frita. Both were very good. We split a secondi which was the special Berkshire pork secreto, which was described as a pork skirt steak with garlic potato pur√©e and romanesco broccoli (incorrectly described as a romanesco sauce). Some tables received bread before the meal but for some reason we did not. Management also seems capable for when the tenants in the apartment above began playing basketball she intervened and also comped the tables a dessert. We ordered the apple dumpling with gelato and it was also quite tasty. Portions are not huge which I particularly like because I prefer to order courses. If you are someone who orders just a meal you may leave hungry depending on you appetite. Dining room was mostly younger crowd while the bar was definitely the over 40 set. Would recommend that the naysayers return to see the improvement although I personally don't know the former glory to be able to compare. Easy access to downtown Salem without the crowds on this month of October.\n",
        "        ''',\n",
        "        '''\n",
        "        The food is always great here. The service from both the manager as well as the staff is super. Only draw back of this restaurant is it's super loud. If you can, snag a patio table!\n",
        "        ''',\n",
        "        '''\n",
        "        This place used to be a cool, chill place. Now its a bunch of neanderthal bouncers hopped up on steroids acting like the can do whatever they want. There are so many better places in davis square where they are glad you are visiting their business. Sad that the burren is now the worst place in davis.\n",
        "        '''\n",
        "        ]\n",
        "\n",
        "\n",
        "tokenizerd_text = tokenizer(corpus)\n",
        "max_length = 50\n",
        "group_tokenizerd_text = group_texts(tokenizerd_text,max_length)\n",
        "\n",
        "my_dataset = CustomDataset(group_tokenizerd_text)\n",
        "batch_size = 2\n",
        "data_loader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "softmask_model = NNSoftmask()\n"
      ],
      "metadata": {
        "id": "UKns-MzU8Gzi"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before pre-training, we need to calculate the importance of the units in each layer. The method to compute this importance is based on the distance between representations derived from the same input (refer to the aforementioned papers for details). Once calculated using the gradient, we then normalize the importance."
      ],
      "metadata": {
        "id": "4mV53TaU8SZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillKL(nn.Module):\n",
        "    def __init__(self, T):\n",
        "        super(DistillKL, self).__init__()\n",
        "        self.T = T\n",
        "\n",
        "    def forward(self, y_s, y_t):\n",
        "        p_s = F.log_softmax(y_s / self.T, dim=1)\n",
        "        p_t = F.softmax(y_t / self.T, dim=1)\n",
        "\n",
        "        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T ** 2) / y_s.shape[0]\n",
        "        return loss\n",
        "\n",
        "def initial_impt():\n",
        "\n",
        "    n_encoder_layer, fc1_size, fc2_size = 1, 30, 10\n",
        "\n",
        "    fc1_impt = torch.zeros(n_encoder_layer, fc1_size)\n",
        "    fc1_mask = torch.ones(n_encoder_layer, fc1_size)\n",
        "    fc1_mask.requires_grad_(requires_grad=True)\n",
        "\n",
        "    fc2_impt = torch.zeros(n_encoder_layer, fc2_size)\n",
        "    fc2_mask = torch.ones(n_encoder_layer, fc2_size)\n",
        "    fc2_mask.requires_grad_(requires_grad=True)\n",
        "\n",
        "    tot_tokens = 0.0\n",
        "\n",
        "    return  fc1_impt, fc1_mask, fc2_impt, fc2_mask, tot_tokens\n",
        "\n",
        "\n",
        "fc1_impt, fc1_mask, \\\n",
        "fc2_impt, fc2_mask, tot_tokens = initial_impt()\n",
        "\n",
        "\n",
        "duplicate_model = NNSoftmask()\n",
        "duplicate_model.return_representation = True\n",
        "softmask_model.return_representation = True\n",
        "kd_loss = DistillKL(1)\n",
        "\n",
        "# before post-train, we compute the importance\n",
        "for step, batch in enumerate(data_loader):\n",
        "  input_ids = batch['idx'].long()\n",
        "  labels = batch['labels']\n",
        "\n",
        "  outputs = softmask_model(input_ids, fc1_mask, fc2_mask)\n",
        "  duplicate_outputs = duplicate_model(input_ids, fc1_mask, fc2_mask)\n",
        "\n",
        "  loss = kd_loss(duplicate_outputs, outputs)  # no need for mean\n",
        "\n",
        "  loss.backward() # compute the gradient\n",
        "\n",
        "  fc1_impt += fc1_mask.grad.clone().detach()\n",
        "  fc2_impt += fc2_mask.grad.clone().detach()\n",
        "\n",
        "  tot_tokens += input_ids.numel()\n",
        "\n",
        "\n",
        "fc1_impt /= tot_tokens\n",
        "fc2_impt /= tot_tokens\n",
        "\n",
        "# Normalize the importance\n",
        "\n",
        "def impt_norm(impt):\n",
        "    tanh = torch.nn.Tanh()\n",
        "    for layer in range(impt.size(0)):\n",
        "        impt[layer] = (impt[layer] - impt[layer].mean()) / impt[\n",
        "            layer].std()  # 2D, we need to deal with this for each layer\n",
        "    impt = tanh(impt).abs()\n",
        "\n",
        "    return impt\n",
        "\n",
        "\n",
        "fc1_impt = impt_norm(fc1_impt)\n",
        "fc2_impt = impt_norm(fc2_impt)\n",
        "\n",
        "print('fc1_impt: ',fc1_impt)\n",
        "print('fc2_impt: ',fc2_impt)\n",
        "\n",
        "print('fc1_impt size: ',fc1_impt.size())\n",
        "print('fc2_impt size: ',fc2_impt.size())\n",
        "\n",
        "print('fc1_impt usage: ', (fc1_impt.sum() / fc1_impt.numel()).item())\n",
        "print('fc2_impt usage: ', (fc2_impt.sum() / fc2_impt.numel()).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQG0dep78bnp",
        "outputId": "ef1b744d-5542-470c-be10-3973ec0a6b29"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1_impt:  tensor([[0.3814, 0.6820, 0.8964, 0.6379, 0.3650, 0.1307, 0.0273, 0.4998, 0.6751,\n",
            "         0.9581, 0.2717, 0.9003, 0.6448, 0.6812, 0.5513, 0.6939, 0.7003, 0.0811,\n",
            "         0.5932, 0.5958, 0.3884, 0.1301, 0.9925, 0.5140, 0.6474, 0.5664, 0.7002,\n",
            "         0.1659, 0.9535, 0.6895]])\n",
            "fc2_impt:  tensor([[0.6698, 0.6477, 0.9137, 0.6336, 0.6296, 0.6755, 0.4832, 0.3186, 0.9397,\n",
            "         0.3872]])\n",
            "fc1_impt size:  torch.Size([1, 30])\n",
            "fc2_impt size:  torch.Size([1, 10])\n",
            "fc1_impt usage:  0.5571809411048889\n",
            "fc2_impt usage:  0.6298627257347107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can begin our training process, applying soft-masking to the gradients."
      ],
      "metadata": {
        "id": "W9WFNRzp8hzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCELoss()\n",
        "softmask_model.return_representation = False\n",
        "optimizer = optim.Adam(softmask_model.parameters(), lr=0.003)\n",
        "epochs = 10\n",
        "# before post-train, we compute the importance\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  i = 0\n",
        "  for step, batch in enumerate(data_loader):\n",
        "    i += 1\n",
        "    if i % 100 == 0:\n",
        "        print(f'Training loss at step {i}: {running_loss/(i*batch_size)}')\n",
        "    input_ids = batch['idx'].long()\n",
        "    labels = batch['labels']\n",
        "\n",
        "    outputs = softmask_model(input_ids)\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    fc1_mask = (1 - fc1_impt[0])\n",
        "    fc2_mask = (1 - fc2_impt[0])\n",
        "\n",
        "    # soft-mask the network\n",
        "    softmask_model.fc1.weight.grad *= fc1_mask.unsqueeze(1)\n",
        "    softmask_model.fc1.bias.grad *= fc1_mask\n",
        "\n",
        "    softmask_model.fc2.weight.grad *= fc2_mask.unsqueeze(1)\n",
        "    softmask_model.fc2.bias.grad *= fc2_mask\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(f'Training loss at step {i}: {running_loss / (len(data_loader) * batch_size)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nzB4gMp8kOE",
        "outputId": "8266a399-3d05-4886-bc57-87a6268813cf"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss at step 1: 0.07949668169021606\n",
            "Training loss at step 2: 0.10997052118182182\n",
            "Training loss at step 3: 0.14041584730148315\n",
            "Training loss at step 4: 0.22584938257932663\n",
            "Training loss at step 1: 0.03280102461576462\n",
            "Training loss at step 2: 0.1690192148089409\n",
            "Training loss at step 3: 0.1968135815113783\n",
            "Training loss at step 4: 0.23040767572820187\n",
            "Training loss at step 1: 0.03295372426509857\n",
            "Training loss at step 2: 0.11730176210403442\n",
            "Training loss at step 3: 0.20168815553188324\n",
            "Training loss at step 4: 0.23493972420692444\n",
            "Training loss at step 1: 0.08686286956071854\n",
            "Training loss at step 2: 0.11723855882883072\n",
            "Training loss at step 3: 0.1989743784070015\n",
            "Training loss at step 4: 0.23041529953479767\n",
            "Training loss at step 1: 0.08122346550226212\n",
            "Training loss at step 2: 0.11534951999783516\n",
            "Training loss at step 3: 0.20059532299637794\n",
            "Training loss at step 4: 0.2298785950988531\n",
            "Training loss at step 1: 0.030641980469226837\n",
            "Training loss at step 2: 0.1110301986336708\n",
            "Training loss at step 3: 0.14278142526745796\n",
            "Training loss at step 4: 0.22969480976462364\n",
            "Training loss at step 1: 0.03186904639005661\n",
            "Training loss at step 2: 0.06296653300523758\n",
            "Training loss at step 3: 0.1494382694363594\n",
            "Training loss at step 4: 0.23010432720184326\n",
            "Training loss at step 1: 0.07868233323097229\n",
            "Training loss at step 2: 0.11035255342721939\n",
            "Training loss at step 3: 0.1410465519875288\n",
            "Training loss at step 4: 0.22869024984538555\n",
            "Training loss at step 1: 0.08288835734128952\n",
            "Training loss at step 2: 0.11522025987505913\n",
            "Training loss at step 3: 0.19670092687010765\n",
            "Training loss at step 4: 0.22550999373197556\n",
            "Training loss at step 1: 0.07949310541152954\n",
            "Training loss at step 2: 0.1616228222846985\n",
            "Training loss at step 3: 0.19323531538248062\n",
            "Training loss at step 4: 0.21883702464401722\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}